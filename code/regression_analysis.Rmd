---
title: "Self Promotion in US Congressional Tweets: A mixed-effects regression analysis"
output: html_document
---

```{r  warning=F, results='hide', message=F} 
require(tidyverse) # dplyr (filter, arrange, mutate, summarize, group_by)
require(data.table) # for convenient data manipulation [i, j, by] (eg, .N, by=groupby_var)
require(lme4) # linear mixed-effects regression: glmer (allow binary dependent variable)
require(ggeffects) # estimated marginal means (that is, expected mean) from regression models
#https://www.cscu.cornell.edu/news/statnews/93_emmeans.pdf

require(sjPlot) # for plotting Odds Ratios of the model

require(broom.mixed) # has a function "augment()" to add model statistics to the data so that we can plot residuals (one task of diagnosing the assumptions of linear regression model)

require(scales) # needed in ggplot > scale_x_date(date_format(..))
require(zoo) # for as.yearmon()
require(tictoc) # timer: tic(); ...; toc()
```


## setup whether to use log-transformation for two independent variables: num_terms, daily_tweets
- finally, we chose the original or raw scale of "num_terms" and "daily_tweets" based on AIC (the one with the smallest AIC)

```{r}
use_log_form_of_num_terms = F
#use_log_form_of_num_terms = T

use_log_form_of_daily_tweets = F
#use_log_form_of_daily_tweets = T
```

## Util functions:
```{r}
folder_of_results = 'working/result/'
ifelse(!dir.exists(folder_of_results), dir.create(folder_of_results, recursive=TRUE), FALSE)

create_filename_for_output = function(namebase, ext='.pdf') {
  terms_str = paste0('__with_terms', ifelse(use_log_form_of_num_terms, 'LOG', 'RAW'))
  tweets_str = paste0('__tweets', ifelse(use_log_form_of_daily_tweets, 'LOG', 'RAW'))
  filename = paste(c(namebase, terms_str, tweets_str, '.', ext), collapse = '')
  return (paste0(folder_of_results, filename))
}

save_fig = function(figname, w = 5, h = 3) {
  ggsave(figname, plot=last_plot(), width=w, height=h)
  embedFonts(figname, outfile = figname)
  print(paste0('- file saved to: ', figname))
}

run_regression_model = function(data, print_summary=T, show_tictoc=T, fixed_factors=NULL) {
  if (is.null(fixed_factors)) {
    fixed_parts1 = "gender + party + chamber + age"
    fixed_part_num_terms = ifelse(use_log_form_of_num_terms, "num_terms_log", "num_terms")
    fixed_part_daily_tweets = ifelse(use_log_form_of_daily_tweets, "daily_tweets_log", "daily_tweets")
    
    fixed_parts2 = paste(c(fixed_part_num_terms, fixed_part_daily_tweets, "followers_log"), collapse = '+')
    fixed_parts = paste(c(fixed_parts1, fixed_parts2), collapse = '+')
  } else {
    fixed_parts = fixed_factors
  }
  
  random_parts = "(1|bio_id) + (1|date)"
  
  formula = as.formula(paste("self_promotion_as_predicted_by_BERT_model ~ ", paste(c(fixed_parts, random_parts), collapse="+")))
  
  if (show_tictoc) tic()
  
  m = glmer(formula, data=data, family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ=0)
  
  if (show_tictoc) toc()

  if (print_summary) {
    print(summary(m), digits=5)
  }

  return(m)
}

```



## loading data (each row is a tweet observation)
- self_promotion_as_predicted_by_BERT_model: 
  (dependent variable) 
  value 1: predicted by BERT model as self-promotion; 
  value 0: predicted as not
- bio_id: the BioGuide ID of the Congress person who posted the tweet
- date: tweet data, in the format of: 2020-01-01
- gender: F, M
- party: D, R
- chamber: house, senate
- age: (tweet_date - birthday) / 365.25
- num_terms: how many terms served in Congress at the time of posting tweet
- followers_log: logarithm of followers (log-transform: normalize highly skewed distribution)

```{r}
# the following data is generated in "_util/util.py"

data_fpath = '../data/final_data_for_regression_analysis.csv.adjust' # the values of age and num_terms for a tweet are no longer fixed (depending on when the tweet was posted)

df = read.csv(data_fpath)
print(nrow(df))

# setDT: convert df from data.frame to data.table
# so that we can conveniently use data.table functions data.table[i, j, by]
setDT(df)

df[, ym:=substr(date, 1, 7)] # get yyyy-mm from date yyyy-mm-dd

df[, daily_tweets:=.N, by = list(bio_id, date)]

df$num_terms_log = log1p(df$num_terms)
df$daily_tweets_log = log1p(df$daily_tweets)

df$date = as.Date(df$date)

# set "M" as reference for gender. Default is 'F'
df <- within(df, gender <- relevel(gender, ref = 'M'))
```


## gender, chamber, party distribution
```{r}
df <- within(df, gender <- relevel(gender, ref = 'M'))
dat = copy(unique(df, by='bio_id'))
dat[, .(count = .N), by = c('gender', 'chamber', 'party')][order(-gender, chamber, party)]
```


## run the analysis on the whole data from 2017-07 to 2021-03
```{r}
model_overall = run_regression_model(df)
```

```{r}

model2tex = function(model) {
  summ = summary(model)
  AIC = summ$AICtab[1]
  coef = summ$coefficients
  
  #sep = '@{\\,\\,\\,\\,\\,}' # if needs to compress space
  sep = ''
  out = c(sprintf('\\begin{tabular}{@{}l@{}r%sr%sl@{}} \n \\hline \n & Coef & Std Err & P-value \\\\ \\hline \n', sep, sep))

  #digits = 3
  for (i in 2:nrow(coef)) {
    se = sprintf("%.3f", coef[i, 2])
    se = paste0('$', se, '$')
    
    estimate = sprintf("%.3f", coef[i, 1])
    pvalue = coef[i, 4]
    
    # bold font for significance
    if (pvalue<0.05) {
      estimate = paste0('$\\mathbf{', estimate, '}$')
    } else {
      estimate = paste0('$', estimate, '$')
    }
    
    mark = ifelse(pvalue<0.001, '***', ifelse(pvalue<0.01, '**', ifelse(pvalue<0.05, '*', ifelse(pvalue<0.1, '\\cdot', ''))))
    if (pvalue<0.05)
      mark = sprintf("^{%s}", mark)

    pv = sprintf("$%.3f\\, %s$", pvalue, mark)


    name = rownames(coef)[i]
    if (name=='genderF')
      name = 'gender [F]'
    else if (name=='partyR')
      name = 'party [R]'
    else if (name=='chambersenate')
      name = 'chamber [senate]'
    else
      name = gsub('_', '\\\\_', name)
    
    line = paste(c(name, estimate, se, pv), collapse = ' & ')
    out = c(out, paste0(line, '\\\\'))
  }
  out = c(out, paste0('\\hline AIC & $', as.integer(AIC), '$ & & \\\\'))
  out = c(out, paste0('Num. obs. & $', nrow(df), '$ & & \\\\'))
  out = c(out, "\\hline \\multicolumn{4}{r}{\\footnotesize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$; $^{.}\\, p<0.1$}}\n\\end{tabular}")

  result = paste(out, collapse = ' \n')
  result = paste0(result)
  return(result)
}

tex_out = model2tex(model_overall)
cat(tex_out)

fpath_model_effects_tex = create_filename_for_output('table_model_effects', 'tex')
sink(fpath_model_effects_tex)
cat(tex_out)
sink()

```


```{r}
use_age_by_10 = T
if (use_age_by_10) {
  df$age_by_10 = df$age / 10
  
  formula = 'self_promotion_as_predicted_by_BERT_model ~ gender + party + chamber + age_by_10 + num_terms + daily_tweets + followers_log + (1 | bio_id) + (1 | date)'
  m2 = glmer(formula, data=df, family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ=0)
  age_str = '_age10'
} else {
  m2 = model_overall
  age_str = ''
}

plot_model(m2, show.values = TRUE, value.offset = .4)
#fpath_output = paste(c(folder_of_results, 'model_odds_ratios', age_str, '.pdf'), collapse = '')
#save_fig(fpath_output, w=3.5, h=3.5) # font "ZapfDingbats" can't be embedded with embedFonts()
fpath_output = paste(c(folder_of_results, 'model_odds_ratios', age_str, '.png'), collapse = '')
ggsave(fpath_output, plot=last_plot(), width=3.5, height=3.5)
```


## compute the expected mean for F/M over the time with ggemeans(), a ggeffects function 

For categorical predictors (chamber, party), ggemmeans() averages over the proportions of the categories of factors.

For continuous predictors (age, num_terms_log, followers_log, daily_tweets_log), they are held constant at their mean value (e.g., age=60).

```{r}
compute_expected_mean_for_month = function(model, term, ym) {
    res = ggemmeans(model, terms = c(term))
    tmp = data.table(res)[, .(x, predicted)] 
    tmp$ym = ym
    tmp$ym = as.Date(as.yearmon(tmp$ym))
    colnames(tmp) = c(term, 'proba', 'ym')
    return(tmp)
}

get_ym_list_for_years = function() {
  yms = c()
  for (year in c('2017', '2018', '2019', '2020', '2021')) {
    if (year=='2017') {
      months = 7:12
    } else if (year=='2021') {
      months = 1:3
    } else {
      months = 1:12
    }
    yms_ = paste(year, sprintf("%02d", months), sep='-')
    yms = c(yms, yms_)
  }
  return (yms)
}
```

## divide data by year-month for plotting gender difference over the each month
```{r}
df_out = NULL

for (ym_ in get_ym_list_for_years()) {
  data = df[ym==ym_]
  cat(paste0(ym_, ' '))
  model = run_regression_model(data, print_summary=F, show_tictoc=F)
  df_out = rbind(df_out, compute_expected_mean_for_month(model, "gender", ym_))
}
```


## plot the trend
```{r}
data = df_out
data$ym = as.Date(as.yearmon(data$ym))
data <- within(data, gender <- relevel(gender, ref = 'F'))

ggplot(data = data, aes(x=ym, y=proba, color=gender)) + geom_line(size=.9, aes(linetype=gender, color=gender)) + xlab('') + ylab('Expected mean of self-promotion') +  scale_linetype_manual(values=c("twodash", "solid")) + scale_x_date(labels = date_format("%b %Y")) + theme(plot.margin=margin(t = 2, r = 2, b = -6, l = 2, unit = "pt")) + theme(legend.position = c(0.01, 0.01), legend.justification = c("left", "bottom"))


fpath_output = create_filename_for_output('trend_gender_difference', 'pdf')
save_fig(fpath_output, w=5, h=2.9)
```

## For Appendix 2 (distribution of 4 numerical values): 
- age
- num_terms  / num_terms_log
- daily_tweets / daily_tweets_log
- followers_log
```{r}

dfu_ = copy(unique(df, by=c('bio_id', 'ym')))
dfu_[, avg_daily_tweets:=median(daily_tweets), by='bio_id']
dfu_[, avg_num_terms:=median(num_terms), by='bio_id']
dfu_[, avg_age:=median(age), by='bio_id']

dfu = unique(dfu_, by=c('bio_id'))
#dfu$avg_daily_tweets_log = log1p(dfu$avg_daily_tweets)
#dfu$avg_daily_tweets_sqrt = sqrt(dfu$avg_daily_tweets)


cbPalette <- c("#56B4E9", "#E69F00", "#F55E00",  "#0072B2",  "#CC79A7", "#BBBBBB", '#666666')

#mylist <- list('age'=5, 'followers_log'=.5, 'num_terms'=1,  'num_terms_log'=.5, 'avg_daily_tweets'=20, 'avg_daily_tweets_log'=.15, 'avg_daily_tweets_sqrt'=.5)

mylist <- list('avg_age'=5, 'followers_log'=.5, 'avg_num_terms'=1, 'avg_daily_tweets'=1)
#mylist <- list('avg_daily_tweets'=1)

for (i in seq_along(mylist)) {
  var = names(mylist)[i]
  bw = mylist[[i]]

  plt = ggplot(dfu, aes_string(x=var, fill='gender')) + geom_histogram(binwidth=bw, alpha=.4, position="identity")  + scale_fill_manual(values=cbPalette) + theme(legend.position = c(0.98, 0.98), legend.justification = c("right", "top"))
  print(plt)
  
  fpath_output = paste(c(folder_of_results, 'distri_', var, '.pdf'), collapse = '')
  save_fig(fpath_output, w=4, h=2.5)
}
```



## Diagnosis of the regression model with one month of data (which is about 50,000)

- http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/

```{r}
data = df[ym=='2020-06',]

factors = "gender + party + chamber + age + num_terms + daily_tweets + followers_log"

m = run_regression_model(data, fixed_factors = factors)
```


## check the assumption that there is no multicollinearity.
- vif: variance inflation factor: an indicator of how much of the inflation of the standard error could be caused by collinearity).
- As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.
```{r}
car::vif(m)
```


## check the linearity assumption for numerical variables
```{r}
probabilities <- predict(m, type = "response")
data_numeric = data[, .(age, num_terms, daily_tweets, followers_log)]
data_numeric[, logit := log(probabilities/(1-probabilities))]

data_sample = data_numeric[sample(.N, 5000)]

predictors <- colnames(data_sample)
data_sample_long = gather(data_sample, key = "predictors", value = "predictor.value", -logit)

ggplot(data_sample_long, aes(logit, predictor.value)) +
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

## check residuals: plot residual and cook's distance
```{r}
model.data <- augment(m) %>% mutate(index = 1:n())
# model.data$index <- seq.int(nrow(model.data))

ggplot(model.data, aes(index, .resid)) + geom_point(aes(color = self_promotion_as_predicted_by_BERT_model), alpha = .05) + theme_bw()

ggplot(model.data, aes(index, .cooksd)) + geom_point(alpha = .2) + theme_bw()
```

## using package DHARMa to diagnose residuals, including normality of errors

```{r}
# https://github.com/florianhartig/DHARMa/issues/212
# What does it mean if a DHARMa test is significant? #212
# Author of DHARMa: we get the following insight: with a sufficiently large dataset, every DHARMa test will be significant!
# Broadly, if deviations are visually small (though significant), they are likely not a big problem.
# Look at this example:
# testData= createData(sampleSize = 50000, overdispersion = 0.1, family = poisson(), randomEffectVariance = 0)
# There is minimal overdispersion in the data, but because of the large sample size, it's still highly significant.

require(DHARMa)

set.seed(0)
m2 = run_regression_model(data[sample(.N, 15000)], fixed_factors = factors)

simulationOutput <- simulateResiduals(fittedModel = m2, plot = T)

```
